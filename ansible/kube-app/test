#### spark test ###
kubectl -s api:8080 exec -it sm-0 bash

### Stream 
####
spark-shell --jars $(echo /opt/phoenix/*.jar | tr ' ' ',')

import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.catalyst.ScalaReflection
import java.sql.DriverManager
import java.sql.Connection

class SparkJob extends Serializable {

   import org.apache.spark.sql.types._
   import org.apache.spark.sql.functions._
   import org.apache.spark.sql.catalyst.ScalaReflection
   import java.sql.DriverManager
   import java.sql.Connection
   import spark.implicits._

   val i = 0 
   val driver = "org.apache.phoenix.jdbc.PhoenixDriver"
   
   def processRow(msg :String) = {

   }

   def  runJob() = {
     Class.forName(driver)
     val connection:Connection = DriverManager.getConnection("jdbc:phoenix:zk-0.zk")
     
     val sql = "upsert into  test (mykey, mycolumn) values(?, ?)"
     val statement = connection.prepareStatement(sql)
     statement.setInt(1,3)
     statement.setString(2,"It works")
     statement.executeUpdate()
     connection.commit()
     
     val lines = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "kafka-0.kafka:9092").option("subscribe", "message").load().selectExpr("CAST(value AS STRING)").as[String]

     val writer = new org.apache.spark.sql.ForeachWriter[String]{
        override def open(partitionId: Long, version: Long) = { println ("Opening"); true }
        override def process(msg: String) = {
          processRow(msg)
          
     }
     override def close(errorOrNull: Throwable) = {}
   }

   lines.writeStream.queryName("Testing").foreach(writer).start
   }
}


sc.addJar("/opt/phoenix/phoenix-4.9.0-HBase-1.2-client.jar")
new SparkJob().runJob
##########
val wordCounts = lines.flatMap(_.split(" ")).groupBy("value").count()
val query = wordCounts.writeStream.outputMode("complete").format("console").start()

### https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-structured-streaming.html
##https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala

val dataSet = List((2,"kalu"),(3,"khan"))
sc.parallelize(dataSet)
sc.parallelize(dataSet).saveToPhoenix("test",Seq("mykey","mycolumn"), zkUrl = Some("zk-0.zk:2181"))
