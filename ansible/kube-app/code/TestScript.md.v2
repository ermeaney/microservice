###hbase

kubectl -s api:8080 exec -it hm-0 bash

hbase shell

create 'users','info'
put 'users','reza','info:name','reza'
put 'users','reza','info:age','30'
get 'users', 'reza'

flush 'users'
###phx

sqlline.py zk-0.zk
create table test (mykey integer not null primary key, mycolumn varchar);
upsert into test values (1,'Hello');
upsert into test values (2,'World!');
select * from test;

### Zeppline

default.driver : org.apache.phoenix.jdbc.PhoenixDrive
default.url : jdbc:phoenix:zk-0.zk:/hbase


##### kafka test ####
kubectl -s api:8080 exec -it kafka-0 bash

## create topic
kafka-topics.sh --create --zookeeper zk-0.zk:2181 --replication-factor 1 --partitions 1 --topic message
kafka-topics.sh --list --zookeeper zk-0.zk:2181


## run producer
kafka-console-producer.sh --broker-list kafka-0.kafka:9092 --topic message

kafka-console-consumer.sh --topic message --from-beginning --zookeeper zk-2.zk:2181

#### hdfs test####
kubectl -s api:8080 exec -it nn-0 bash

#### spark test ###
kubectl -s api:8080 exec -it sm-0 bash

spark-shell --master spark://sm-0.sprkmstr:7077

val textFile = sc.textFile("hdfs://nn-0.nn:9000/data.txt")
textFile.collect().foreach(println)


### Stream 
####
spark-shell --jars $(echo /opt/phoenix/*.jar | tr ' ' ',')

import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.catalyst.ScalaReflection
import java.sql.DriverManager
import java.sql.Connection

class SparkJob extends Serializable {

import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.catalyst.ScalaReflection
import java.sql.DriverManager
import java.sql.Connection
import spark.implicits._

   val driver = "org.apache.phoenix.jdbc.PhoenixDriver"
   val zk="zk-0.zk"
   Class.forName(driver)
   val connectionString = s"jdbc:phoenix:$zk"
   val connection:Connection = DriverManager.getConnection(connectionString)
   val sql = "upsert into  test (mykey, mycolumn) values(?, ?)"

   def processRow(msg :String) = {
      val statement = connection.prepareStatement(sql)
      statement.setInt(1,3)
      statement.setString(2,msg)
      statement.executeUpdate()
      connection.commit() 
   }

   def  runJob() = {
     val lines = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "kafka-0.kafka:9092").option("subscribe", "message").load().selectExpr("CAST(value AS STRING)").as[String]
     val writer = new org.apache.spark.sql.ForeachWriter[String]{
        override def open(partitionId: Long, version: Long) = { println ("Opening"); true }
        override def process(msg: String) = {
          println(msg )
     }
     override def close(errorOrNull: Throwable) = {}
   }

   lines.writeStream.queryName("Testing").foreach(writer).start
   }
}


##########
val wordCounts = lines.flatMap(_.split(" ")).groupBy("value").count()
val query = wordCounts.writeStream.outputMode("complete").format("console").start()

### https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-structured-streaming.html
##https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala

val dataSet = List((2,"kalu"),(3,"khan"))
sc.parallelize(dataSet)
sc.parallelize(dataSet).saveToPhoenix("test",Seq("mykey","mycolumn"), zkUrl = Some("zk-0.zk:2181"))
